{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "`pip`\n",
    "\n",
    "```\n",
    "pip install git+https://github.com/d2cml-ai/csdid-pyspark\n",
    "pip install git+https://github.com/d2cml-ai/drdid\n",
    "pip install pyspark, findspark\n",
    "```\n",
    "\n",
    "`pipenv`\n",
    "\n",
    "```\n",
    "#csdid\n",
    "pipenv install -e git+https://github.com/d2cml-ai/csdid-pyspark#egg=csdidspark\n",
    "pipenv install --editable git+https://github.com/d2cml-ai/drdid#egg=drdid\n",
    "pipenv install pyspark, findspark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Configuration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- `.config('spark.master', 'local[4]')`: Local execution with 4 CPU cores.\n",
    "- `.config('spark.executor.memory', '1g')`: Memory allocation for each cluster.\n",
    "- `.config('spark.driver.cores', '5')`: Sets the number of CPU cores to be allocated to the driver.\n",
    "- `.config('spark.rdd.compress', True)`: Enables RDD (Resilient Distributed Dataset) compression to save memory space and enhance performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import warnings\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import  SparkSession\n",
    "spark = SparkSession.builder.appName('pyspark-csdid')\\\n",
    "    .config('spark.master', 'local[4]')\\\n",
    "    .config('spark.executor.memory', '1g')\\\n",
    "    .config('spark.driver.cores', '5')\\\n",
    "    .config('spark.rdd.compress', True)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "## Download "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os, requests\n",
    "datasets_online = [\n",
    "\t'https://www.dropbox.com/scl/fi/e6xzb1nsiqixnhx7pvptn/5g10t.csv?rlkey=nalduzdksxxohgrth69fberlq&dl=1',\n",
    "\t'https://www.dropbox.com/scl/fi/edglr6xerr39mk8ch3ey7/5g40t.csv?rlkey=kchelnkdmy9kocl6ynu4aodsw&dl=1',\n",
    "\t'https://www.dropbox.com/scl/fi/twoqqb97mn6fo22dmdjbq/20g40t.csv?rlkey=imphdyf0k4nex00hdwotuvfsj&dl=1'\n",
    "]\n",
    "\n",
    "data_names = ['5g10t', '5g40t', '20g40t']\n",
    "\n",
    "data_path = 'data'\n",
    "data_names1 = [data_path + '/' + names + '.csv' for names in data_names]\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "\tos.mkdir(data_path)\n",
    "\n",
    "def download_data(_from, _to):\n",
    "\tif os.path.exists(_to):\n",
    "\t\treturn\n",
    "\tdata_ = requests.get(_from)\n",
    "\twith open(_to, 'wb') as f:\n",
    "\t\tf.write(data_.content)\n",
    "\n",
    "for i, j in zip(datasets_online, data_names1):\n",
    "\t# print(i, j)\n",
    "\tdownload_data(i, j)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSdid with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csdids.ATTgt import ATTgt\n",
    "def csdid_estimate(data_path):\n",
    "\tyname, gname, idname, tname = 'Y', 'G', 'id', 'period'\n",
    "\tdata = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\tattgt = ATTgt(data=data, tname=tname, gname=gname, yname=yname, idname=idname)\n",
    "\tattgt.fit(bstrap=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Groups, 1 Milllon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:35<00:00,  3.97s/it]\n",
      "100%|██████████| 9/9 [00:34<00:00,  3.87s/it]\n",
      "100%|██████████| 9/9 [00:34<00:00,  3.85s/it]\n",
      "100%|██████████| 9/9 [00:34<00:00,  3.81s/it]\n",
      "100%|██████████| 9/9 [00:34<00:00,  3.82s/it]\n",
      "100%|██████████| 5/5 [02:53<00:00, 34.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 22s\n",
      "Wall time: 3min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "csdid_estimate(data_names1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Groups, 4 Milllons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "csdid_estimate(data_names1[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20 Groups, 4 Milllons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "csdid_estimate(data_names1[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testing_packages-TyrHTNW5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
